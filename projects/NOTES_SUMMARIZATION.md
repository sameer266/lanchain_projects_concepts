

````markdown
# ğŸ“˜ PDF Q&A Bot (DeepSeek + LangChain + FAISS)

This project allows you to **ask questions about PDF documents** using a local LLM (DeepSeek) with **RAG (Retrieval-Augmented Generation)**. The bot reads your PDFs, finds relevant text, and answers your questions accurately.

---

## ğŸ§© Features

- Load **single or multiple PDFs** from a folder
- Split PDFs into **small chunks** for better processing
- Convert text chunks into **vectors** (numeric representation of meaning)
- Store vectors in **FAISS vector database** for fast search
- Use **DeepSeek LLM** to generate **human-readable answers**
- Command-line interface (CLI) for interactive Q&A

---

## ğŸ—ï¸ How It Works (Flow)

```text
[PDF(s)]
   â†“  (split into chunks)
[Text chunks]
   â†“  (convert to vectors)
[Vector Database (FAISS)]
   â†“  (search relevant chunks)
[Relevant chunks]
   â†“  (DeepSeek LLM)
[Answer to User]
````

**Step-by-step:**

1. **Load PDF(s)** â†’ Read all PDF text into Python.
2. **Split into chunks** â†’ Each chunk is small enough for the LLM to process.
3. **Convert to vectors** â†’ Turn text into numbers that represent meaning.
4. **Store in vector DB** â†’ FAISS keeps vectors for **fast similarity search**.
5. **Ask a question** â†’ Vector DB finds the **most relevant chunks**.
6. **DeepSeek LLM generates answer** â†’ Produces human-readable response.

---

## âš™ï¸ Installation

1. Create and activate a virtual environment (Python 3.10 or 3.11 recommended):

```bash
python -m venv venv
venv\Scripts\activate      # Windows
# source venv/bin/activate # Linux / Mac
```

2. Install required packages:

```bash
pip install langchain langchain_community langchain-core
pip install pypdf faiss-cpu
```

---

## ğŸ§‘â€ğŸ’» Project Structure

```
pdf_qa_bot/
â”‚
â”œâ”€â”€ pdf_qa.py           # Main script
â”œâ”€â”€ pdfs/               # Folder for PDF files
â”‚   â”œâ”€â”€ sample1.pdf
â”‚   â””â”€â”€ sample2.pdf
â””â”€â”€ README.md
```

---

## ğŸ’» Code (`pdf_qa.py`)

```python
from langchain_community.llms import Ollama
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
import os

# 1ï¸âƒ£ Load all PDFs from folder
pdf_folder = "pdfs"
documents = []
for file in os.listdir(pdf_folder):
    if file.endswith(".pdf"):
        loader = PyPDFLoader(os.path.join(pdf_folder, file))
        documents.extend(loader.load())

# 2ï¸âƒ£ Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,   # each chunk = 500 characters
    chunk_overlap=100 # repeat 100 characters for context
)
chunks = text_splitter.split_documents(documents)

# 3ï¸âƒ£ Create embeddings
embeddings = OllamaEmbeddings(model="deepseek-r1:1.5b")

# 4ï¸âƒ£ Create FAISS vector database
vectorstore = FAISS.from_documents(chunks, embeddings)

# 5ï¸âƒ£ Load DeepSeek LLM
llm = Ollama(
    model="deepseek-r1:1.5b",
    temperature=0.2
)

# 6ï¸âƒ£ Create RetrievalQA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)

# 7ï¸âƒ£ CLI loop
print("ğŸ“˜ PDF Q&A Bot (type 'exit' to quit)\n")
while True:
    question = input("You: ")
    if question.lower() == "exit":
        print("ğŸ‘‹ Goodbye")
        break
    answer = qa_chain.run(question)
    print("Bot:", answer)
```

---

## ğŸ§  Example Usage

```
You: What are Django signals?
Bot: Django signals let parts of your app respond automatically to events like saving or deleting data.

You: Summarize chapter 2
Bot: Chapter 2 explains models in Django, which define database tables and relationships.
```

---

## ğŸ”¹ Notes

* `chunk_size` â†’ How big each piece of text is (500 characters recommended)
* `chunk_overlap` â†’ Keeps 100 characters repeated for context
* Vector DB = **search tool** for relevant chunks
* LLM (DeepSeek) = **brain that generates human-readable answers**

---

## ğŸš€ Next Improvements

* Save FAISS database to disk â†’ **load without reprocessing PDFs**
* Add memory â†’ remember previous questions
* Add a web interface â†’ Flask / Django app
* Multi-language PDFs â†’ support non-English text

---

## ğŸ¯ Summary

This project demonstrates **RAG (Retrieval-Augmented Generation)** using PDFs and a **local LLM (DeepSeek)**.
It ensures **accurate answers from your own documents** while working offline.

```

---
